#!/usr/bin/env python3
"""
Flux LoRAå¤„ç†è„šæœ¬ - ç”Ÿæˆflux_repair_å¼€å¤´çš„æ–‡ä»¶
åŸºäºå‚è€ƒå›¾ç‰‡å’Œæ ¡æ­£æ ·æœ¬çš„é¢œè‰²åå·®æ¥æŒ‡å¯¼æ‰€æœ‰å›¾ç‰‡çš„å¤„ç†
æ”¯æŒåŠ¨æ€å‚æ•°è°ƒæ•´å’Œæ‰¹é‡å¤„ç†ï¼ˆä¸å¯ç”¨é¢œè‰²çŸ«æ­£ï¼‰
"""

# åœ¨å¯¼å…¥ä»»ä½•å¯èƒ½ä½¿ç”¨OpenBLASçš„åº“ä¹‹å‰è®¾ç½®çº¿ç¨‹æ•°é™åˆ¶
import os
os.environ["OPENBLAS_NUM_THREADS"] = "4"
os.environ["MKL_NUM_THREADS"] = "4"
os.environ["OMP_NUM_THREADS"] = "4"
os.environ["NUMEXPR_NUM_THREADS"] = "4"

import sys
import yaml
import torch
import numpy as np
from pathlib import Path
from PIL import Image
from tqdm import tqdm
import logging
import time
import uuid

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class FluxRepairProcessor:
    """é›†æˆåŠ¨æ€é¢œè‰²æ ¡æ­£çš„Fluxå¤„ç†å™¨ï¼ˆä¸å¯ç”¨é¢œè‰²çŸ«æ­£ï¼‰"""
    
    def __init__(self, config_path="configs/style_reference.yaml"):
        """åˆå§‹åŒ–å¤„ç†å™¨"""
        self.config = self.load_config(config_path)
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.pipeline = None
        
        # æ‰¹æ¬¡ä¿¡æ¯
        self.batch_id = self._generate_batch_id()
        
        self.setup_pipeline()
        
    def _generate_batch_id(self) -> str:
        """ç”Ÿæˆæ‰¹æ¬¡ID"""
        timestamp = int(time.time())
        unique_id = str(uuid.uuid4())[:8]
        return f"batch_{timestamp}_{unique_id}"
    
    def load_config(self, config_path):
        """åŠ è½½é…ç½®æ–‡ä»¶"""
        config_file = Path(__file__).parent / config_path
        if not config_file.exists():
            logger.error(f"é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_file}")
            return None
            
        with open(config_file, 'r', encoding='utf-8') as f:
            return yaml.safe_load(f)
    
    def setup_pipeline(self):
        """è®¾ç½®Flux + LoRA pipeline"""
        if not self.config:
            logger.error("é…ç½®åŠ è½½å¤±è´¥")
            return
            
        logger.info("æ­£åœ¨åŠ è½½Flux LoRA...")
        
        try:
            from diffusers import FluxImg2ImgPipeline
            
            # åŠ è½½åŸºç¡€æ¨¡å‹
            base_model = self.config['model']['base_model']
            logger.info(f"åŠ è½½åŸºç¡€æ¨¡å‹: {base_model}")
            
            self.pipeline = FluxImg2ImgPipeline.from_pretrained(
                base_model,
                torch_dtype=torch.bfloat16,
                use_safetensors=True
            )
            
            # åŠ è½½LoRA
            lora_path = Path(__file__).parent / self.config['model']['lora_path']
            if lora_path.exists():
                logger.info(f"åŠ è½½LoRA: {lora_path}")
                self.pipeline.load_lora_weights(str(lora_path))
                logger.info("âœ… LoRAåŠ è½½æˆåŠŸ")
            else:
                logger.warning(f"LoRAæ–‡ä»¶ä¸å­˜åœ¨: {lora_path}")
            
            # ç§»åŠ¨åˆ°è®¾å¤‡
            self.pipeline = self.pipeline.to(self.device)
            
            # å¯ç”¨å†…å­˜ä¼˜åŒ–
            if self.config.get('advanced', {}).get('enable_memory_efficient', True):
                self.pipeline.enable_model_cpu_offload()
            if self.config.get('advanced', {}).get('enable_attention_slicing', True):
                self.pipeline.enable_attention_slicing()
            
            # æ¸…ç†GPUç¼“å­˜
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
                
            logger.info("âœ… Pipelineè®¾ç½®å®Œæˆ")
            
        except Exception as e:
            logger.error(f"Pipelineè®¾ç½®å¤±è´¥: {e}")
            raise
    
    def process_image(self, input_path, output_path):
        """å¤„ç†å•å¼ å›¾ç‰‡"""
        try:
            # åŠ è½½è¾“å…¥å›¾ç‰‡
            input_image = Image.open(input_path).convert("RGB")
            
            # è°ƒæ•´å›¾ç‰‡å¤§å°
            output_size = self.config['processing']['output_size']
            input_image = input_image.resize((output_size, output_size), Image.Resampling.LANCZOS)
            
            logger.info(f"ğŸ¨ å¼€å§‹å¤„ç†å›¾ç‰‡: {input_path.name}")
            
            # 1. è·³è¿‡é¢œè‰²æ ¡æ­£ï¼ˆä¸å¯ç”¨é¢œè‰²çŸ«æ­£ï¼‰
            color_correction_params = None
            
            # 2. æ„å»ºå¢å¼ºçš„promptï¼ˆç¼©çŸ­ä»¥é¿å…tokenè¶…é™ï¼‰
            base_prompt = "v3ct0r style, simple flat vector art, isolated on white bg, high quality, crisp edges, clean design, professional vector illustration, smooth curves, perfect geometry, minimalist style, modern design, sharp details, clean lines, geometric precision, vector graphics, flat design, clean composition, professional artwork"
            
            # å¢å¼ºçš„è´Ÿé¢prompt
            negative_prompt = "no extra objects, no layout changes, no new patterns, no gradients, no heavy re-stylization, no color shifts, no over-smoothing, no artifacts, no blur, no noise, no pixelation, no jagged edges, no rough textures, no complex backgrounds, no shadows, no 3d effects, no realistic rendering, no photographic elements, no hand-drawn elements, no sketchy lines, no messy details, no watermarks, no text, no logos, no signatures"
            
            # 3. è·å–å¤„ç†å‚æ•°
            processing_config = self.config['processing']
            
            # æ ¹æ®é¢œè‰²æ ¡æ­£å‚æ•°åŠ¨æ€è°ƒæ•´å¤„ç†å‚æ•°ï¼ˆè·³è¿‡ï¼Œå› ä¸ºæ²¡æœ‰é¢œè‰²æ ¡æ­£ï¼‰
            strength = processing_config['strength']
            guidance_scale = processing_config['guidance_scale']
            
            logger.info(f"ğŸ¯ æœ€ç»ˆå¤„ç†å‚æ•°: strength={strength:.2f}, guidance_scale={guidance_scale:.1f}")
            
            # 4. ç”Ÿæˆå›¾ç‰‡
            with torch.autocast(device_type="cuda", dtype=torch.bfloat16, enabled=(self.device.type == "cuda")):
                result = self.pipeline(
                    prompt=base_prompt,
                    negative_prompt=negative_prompt,
                    image=input_image,
                    strength=strength,
                    guidance_scale=guidance_scale,
                    num_inference_steps=processing_config['num_inference_steps'],
                    height=output_size,
                    width=output_size
                )
            
            # ä¿å­˜ç»“æœ
            output_image = result.images[0]
            
            # è°ƒæ•´è¾“å‡ºå›¾ç‰‡å¤§å°ä¸º512x512
            final_output_size = 512
            output_image = output_image.resize((final_output_size, final_output_size), Image.Resampling.LANCZOS)
            
            output_image.save(output_path)
            
            # æ¸…ç†GPUç¼“å­˜
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
            
            # 5. è®°å½•å¤„ç†ä¿¡æ¯
            self._save_processing_info(input_path, output_path, color_correction_params, 
                                     strength, guidance_scale, base_prompt)
            
            logger.info(f"âœ… å¤„ç†å®Œæˆ: {input_path.name} -> {output_path.name}")
            return True
            
        except Exception as e:
            logger.error(f"âŒ å¤„ç†å¤±è´¥ {input_path.name}: {e}")
            return False
    
    def _save_processing_info(self, input_path, output_path, color_correction_params, 
                            strength, guidance_scale, prompt):
        """ä¿å­˜å¤„ç†ä¿¡æ¯"""
        try:
            info_dir = Path(output_path).parent / "processing_info"
            info_dir.mkdir(exist_ok=True)
            
            info_file = info_dir / f"{Path(input_path).stem}_info.json"
            
            info = {
                'batch_id': self.batch_id,
                'input_file': str(input_path),
                'output_file': str(output_path),
                'processing_parameters': {
                    'strength': strength,
                    'guidance_scale': guidance_scale,
                    'num_inference_steps': self.config['processing']['num_inference_steps'],
                    'processing_size': self.config['processing']['output_size'],
                    'final_output_size': 512,
                    'prompt': prompt,
                    'negative_prompt': "no extra objects, no layout changes, no new patterns, no gradients, no heavy re-stylization, no color shifts, no over-smoothing, no artifacts, no blur, no noise, no pixelation, no jagged edges, no rough textures, no complex backgrounds, no shadows, no 3d effects, no realistic rendering, no photographic elements, no hand-drawn elements, no sketchy lines, no messy details, no watermarks, no text, no logos, no signatures"
                },
                'color_correction': {
                    'applied': color_correction_params is not None,
                    'parameters': color_correction_params if color_correction_params else {},
                    'correction_strength': color_correction_params.get('rgb_correction', {}).get('intensity', 0) if color_correction_params else 0
                },
                'timestamp': time.time()
            }
            
            import json
            with open(info_file, 'w', encoding='utf-8') as f:
                json.dump(info, f, indent=2, ensure_ascii=False)
                
        except Exception as e:
            logger.warning(f"âš ï¸ å¤„ç†ä¿¡æ¯ä¿å­˜å¤±è´¥: {e}")
    
    def process_batch(self, input_dir, output_dir):
        """æ‰¹é‡å¤„ç†å›¾ç‰‡"""
        input_path = Path(input_dir)
        output_path = Path(output_dir)
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        output_path.mkdir(parents=True, exist_ok=True)
        
        # è·å–è¾“å…¥å›¾ç‰‡
        image_extensions = ['.png', '.jpg', '.jpeg', '.bmp', '.tiff']
        input_images = []
        
        for ext in image_extensions:
            input_images.extend(input_path.glob(f"*{ext}"))
        
        # æ’é™¤ori_svgç›®å½•
        input_images = [img for img in input_images if img.parent == input_path]
        
        if not input_images:
            logger.error(f"åœ¨ {input_dir} ä¸­æ²¡æœ‰æ‰¾åˆ°å›¾ç‰‡æ–‡ä»¶")
            return
        
        logger.info(f"æ‰¾åˆ° {len(input_images)} å¼ å›¾ç‰‡éœ€è¦å¤„ç†")
        logger.info(f"ğŸ“Š æ‰¹æ¬¡ID: {self.batch_id}")
        
        # æ‰¹é‡å¤„ç†
        success_count = 0
        for input_image in tqdm(input_images, desc="å¤„ç†å›¾ç‰‡"):
            output_filename = f"flux_repair_{input_image.stem}.png"
            output_file = output_path / output_filename
            
            if self.process_image(input_image, output_file):
                success_count += 1
        
        # æ€»ç»“
        logger.info("=" * 60)
        logger.info(f"ğŸ‰ Fluxä¿®å¤å¤„ç†å®Œæˆ!")
        logger.info(f"ğŸ“Š ç»Ÿè®¡ä¿¡æ¯:")
        logger.info(f"   - æ‰¹æ¬¡ID: {self.batch_id}")
        logger.info(f"   - æ€»å›¾ç‰‡æ•°: {len(input_images)}")
        logger.info(f"   - æˆåŠŸå¤„ç†: {success_count}")
        logger.info(f"   - å¤±è´¥æ•°é‡: {len(input_images) - success_count}")
        logger.info(f"   - æˆåŠŸç‡: {success_count/len(input_images)*100:.1f}%")
        logger.info(f"ğŸ“ è¾“å‡ºç›®å½•: {output_path}")
        logger.info(f"ğŸ“ å¤„ç†ä¿¡æ¯: {output_path}/processing_info/")
        logger.info("=" * 60)

def main():
    """ä¸»å‡½æ•°"""
    logger.info("ğŸš€ å¼€å§‹ä½¿ç”¨Flux LoRAå¤„ç†bluecarå›¾ç‰‡...")
    
    # è·¯å¾„é…ç½®
    base_dir = Path(__file__).parent
    input_dir = base_dir / "input" / "your_images" / "bluecar_"
    output_dir = base_dir / "output" / "bluecar_flux_repair"
    config_path = "configs/style_reference.yaml"
    
    # æ£€æŸ¥è¾“å…¥ç›®å½•
    if not input_dir.exists():
        logger.error(f"è¾“å…¥ç›®å½•ä¸å­˜åœ¨: {input_dir}")
        return
    
    # åˆå§‹åŒ–å¤„ç†å™¨
    try:
        processor = FluxRepairProcessor(config_path)
        logger.info("âœ… Fluxä¿®å¤å¤„ç†å™¨åˆå§‹åŒ–æˆåŠŸ")
    except Exception as e:
        logger.error(f"âŒ Fluxä¿®å¤å¤„ç†å™¨åˆå§‹åŒ–å¤±è´¥: {e}")
        return
    
    # æ‰¹é‡å¤„ç†
    processor.process_batch(input_dir, output_dir)

if __name__ == "__main__":
    main()
